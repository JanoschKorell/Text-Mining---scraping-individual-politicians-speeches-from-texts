{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83fc434d-06ed-4ad1-8a8d-79c050acb966",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pickle\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import os\n",
    "import tabula\n",
    "import tabula.io\n",
    "import tabulate\n",
    "import PyPDF2\n",
    "import shutil\n",
    "import _tkinter\n",
    "import pandas as pd\n",
    "from scipy import nan\n",
    "import numpy as np\n",
    "import openpyxl\n",
    "import os\n",
    "import io\n",
    "import re\n",
    "import errno\n",
    "# import ghostscript\n",
    "import locale\n",
    "import warnings\n",
    "import xlrd\n",
    "import xlrd3\n",
    "from itertools import chain\n",
    "import functools\n",
    "import operator\n",
    "import pylab\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "import tabula\n",
    "import tabula.io\n",
    "import tabulate\n",
    "import PyPDF2\n",
    "import _tkinter\n",
    "import pandas as pd\n",
    "from scipy import nan\n",
    "import numpy as np\n",
    "import openpyxl\n",
    "import os\n",
    "import io\n",
    "import re\n",
    "import errno\n",
    "# import ghostscript\n",
    "import locale\n",
    "import warnings\n",
    "import xlrd\n",
    "import xlrd3\n",
    "from itertools import chain\n",
    "import functools\n",
    "import operator\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import pickle\n",
    "from io import StringIO\n",
    "import csv\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "matplotlib.use('AGG')\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "from tika import parser\n",
    "import json\n",
    "import pyperclip, re\n",
    "from itertools import chain\n",
    "import regex\n",
    "pd.options.mode.chained_assignment = None\n",
    "import collections\n",
    "from matplotlib.pyplot import figure\n",
    "import copy\n",
    "import pypdftk\n",
    "from PyPDF2 import PdfFileWriter,PdfFileReader\n",
    "\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import scipy.cluster.hierarchy as sch\n",
    "\n",
    "import mysql.connector\n",
    "import networkx as nx\n",
    "\n",
    "import itertools\n",
    "import click\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "\n",
    "import prince\n",
    "import seaborn as sns\n",
    "import re\n",
    "#import spacy\n",
    "#from spacy import displacy\n",
    "#spacy.cli.download(\"de_core_news_sm\")\n",
    "#spacy.cli.download(\"de_core_news_md\")\n",
    "#from spacy.matcher import Matcher\n",
    "#from spacy.matcher import PhraseMatcher\n",
    "\n",
    "from pathlib import Path\n",
    "#from spacy.tokens import Span\n",
    "\n",
    "#from spacy.symbols import ORTH\n",
    "#from spacy.language import Language\n",
    "\n",
    "#nlp = spacy.load(name='de_core_news_sm')\n",
    "\n",
    "#from Korrekturen_py import Korrekturen_py_func\n",
    "\n",
    "\n",
    "import ssl\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "\n",
    "def show_ents(doc):\n",
    "\n",
    "    sentence = Sentence(doc)\n",
    "\n",
    "    # predict NER tags\n",
    "    tagger.predict(sentence)\n",
    "\n",
    "\n",
    "    preds = [(token.text,token.labels[0].value) for token in sentence.get_spans('ner')]\n",
    "\n",
    "    for key, val in preds:\n",
    "        if val == 'PER':\n",
    "            return key\n",
    "    \n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(name='de_core_news_sm')\n",
    "import nltk\n",
    "\n",
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "\n",
    "\n",
    "from ipynb.fs.full.text_ import replace_delete\n",
    "\n",
    "# load tagger\n",
    "#tagger = SequenceTagger.load(\"flair/ner-german-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8b264b-f762-4830-9977-dacd0c4a56c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/janoschkorell/Desktop/Wissenschaft/Statistik/Python/Masterarbeit/Plenarprotokolle/'\n",
    "os.chdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56848731-9fc1-47a1-9573-141f940a123f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funktionen\n",
    "\n",
    "def Check(filenamePDF):\n",
    "    gehnet_path = path + 'Check'\n",
    "    if not os.path.exists(gehnet_path):\n",
    "        try:\n",
    "            os.mkdir(gehnet_path)\n",
    "        except OSError as error:\n",
    "            if error.errno != errno.EEXIST:\n",
    "                raise\n",
    "\n",
    "    shutil.move(os.path.join(path, filenamePDF), gehnet_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71dbeea1-93df-4100-a0b4-a8a59d825809",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = 1\n",
    "df_raw = pd.read_pickle('df.pkl')\n",
    "df_all = pd.DataFrame({'Redner':[ ], 'Reden':[ ],'Wahlperiode':[ ], 'Sitzung':[ ], 'Datum':[ ]})\n",
    "\n",
    "\n",
    "for plenarpro,filenamePDF in zip(df_raw.loc[:,'docs'], df_raw.loc[:,'filenamePDF']):\n",
    "\n",
    "   \n",
    "    \n",
    "            print(filenamePDF)\n",
    "            #Vor Beginn löschen\n",
    "            regBeginn = r'.*Beginn\\:[\\d\\.\\sUhr\\:]+(\\n\\n?.*)'\n",
    "            Beginn = re.search(regBeginn, plenarpro, re.DOTALL)\n",
    "            plenarpro = Beginn.group(1)\n",
    "\n",
    "            #Find_and_replace\n",
    "            plenarpro = replace_delete(plenarpro)\n",
    "\n",
    "\n",
    "            #Delete from end\n",
    "            plenarpro = re.sub(r'\\((Schluss|Ende)[0-9\\: \\.Uhr]+\\).*', '\\n\\nENDE ENDE (ENDE)', plenarpro, flags=re.DOTALL  | re.MULTILINE)\n",
    "    \n",
    "            with open(filenamePDF+'.txt', 'w') as f:\n",
    "                f.write(plenarpro)\n",
    "                \n",
    "            #Extraction of the speaker with speech\n",
    "            regRedner = r'((\\n\\n[A-ZLÖÄÜ][a-züöäßğéè]+[\\s\\-][A-ZÖÄÜvindè][ a-züöäßéğè\\.]+\\s?([A-ZLÖÄÜ][a-züöäßğéè]+)?\\s?([A-ZÖÄÜvindè\\-\\(\\s]+[A-Z a-züöäßéğè\\.\\)\\-]+)?\\([A-Za-zöäü]{3,12}\\)\\:(?!\\s\\.)|\\n\\n[A-ZLÖÄÜ][a-züöäßğéè]+[\\s\\-][A-ZÖÄÜvindè][ a-züöäßéğè\\.]+\\s?([A-ZLÖÄÜ][a-züöäßğéè]+)?\\s?([\\-\\(]+[A-ZÖÄÜvindè][A-Z a-züöäßéğè\\.\\)\\-]+)?(Parl\\.\\s)?Staatssekretäri?n?[A-Za-zöäü\\s\\n\\-]+\\:(?!\\s\\.)|\\n\\n[A-ZLÖÄÜ][a-züöäßğéè]+[\\s\\-][A-ZÖÄÜvindè][ a-züöäßéğè\\.]{2,22}\\s?([A-ZLÖÄÜ][a-züöäßğéè]+)?\\s?([\\s\\-\\(]+[A-ZÖÄÜvindè][A-Z a-züöäßéğè\\.\\)\\-]{2,22})?(Bundes)?(Staats)?ministeri?n?[ \\nbeidfüeirsm]{2,10}[A-Za-zöäü\\s\\n\\-]{3,60}\\:|\\n\\n(Vize)?(Alters)?P?p?räsidenti?n?\\s[A-ZÖÄÜ][a-züößäèé]+[\\s\\-][A-ZÖÄÜ][a-zßüöäèğé]+([\\s\\-][A-ZÖÄÜ][a-züéßèöğä]+)?\\:|\\n\\n[A-ZÖÄÜ][a-züößäèé]+\\s[A-ZÖÄÜ][a-zßüöäèğé]+\\, Bundeskanzleri?n?\\:).*?)(?=(\\n\\n[A-ZLÖÄÜ][a-züöäßğéè]+[\\s\\-][A-ZÖÄÜvindè][ a-züöäßéğè\\.]+\\s?([A-ZLÖÄÜ][a-züöäßğéè]+)?\\s?([A-ZÖÄÜvindè\\-\\(\\s]+[A-Z a-züöäßéğè\\.\\)\\-]+)?\\([A-Za-zöäü]{3,12}\\)\\:(?!\\s\\.)|\\n\\n[A-ZLÖÄÜ][a-züöäßğéè]+[\\s\\-][A-ZÖÄÜvindè][ a-züöäßéğè\\.]+\\s?([A-ZLÖÄÜ][a-züöäßğéè]+)?\\s?([\\-\\(]+[A-ZÖÄÜvindè][A-Z a-züöäßéğè\\.\\)\\-]+)?(Parl\\.\\s)?Staatssekretäri?n?[A-Za-zöäü\\s\\n\\-]+\\:(?!\\s\\.)|\\n\\n[A-ZLÖÄÜ][a-züöäßğéè]+[\\s\\-][A-ZÖÄÜvindè][ a-züöäßéğè\\.]+\\s?([A-ZLÖÄÜ][a-züöäßğéè]+)?\\s?([\\s\\-\\(]+[A-ZÖÄÜvindè][A-Z a-züöäßéğè\\.\\)\\-]{2,22})?(Bundes)?(Staats)?ministeri?n?[ beideifür\\nsm]{2,10}[A-Za-zöäü\\s\\n\\-]{3,60}\\:|\\n\\n(Vize)?(Alters)?P?p?räsidenti?n?\\s[A-ZÖÄÜ][a-züößäèé]+[\\s\\-][A-ZÖÄÜ][a-zßüöäèğé]+([\\s\\-][A-ZÖÄÜ][a-züéßèöğä]+)?\\:)|\\n\\n[A-ZÖÄÜ][a-züößäèé]+\\s[A-ZÖÄÜ][a-zßüöäèğé]+\\, Bundeskanzleri?n?\\:)'\n",
    "\n",
    "            matchRedner = re.finditer(regRedner, plenarpro, re.DOTALL | re.MULTILINE)\n",
    "            Reden_rawL = []\n",
    "            for Rede in matchRedner:\n",
    "                Rede = Rede.group(0)\n",
    "                Reden_rawL.append(Rede)\n",
    "\n",
    "            #Extraction of the speaker\n",
    "            RednerL = []\n",
    "            for Rede in Reden_rawL:\n",
    "                regRedner = r'(.*?)\\:'\n",
    "                matchRedner = re.search(regRedner, Rede, re.DOTALL)\n",
    "                match = matchRedner.group(1)\n",
    "                match = re.sub(r'\\n', '', match).strip() \n",
    "    \n",
    "    \n",
    "                try:\n",
    "                    regCheck = r'\\.[A-Z]'\n",
    "                    matchCheck = re.search(regCheck, match, re.DOTALL)\n",
    "                    matchCheck = matchCheck.group(0)\n",
    "                    matchCheck = re.sub(r'\\n', '', matchCheck).strip() \n",
    "                    print('\\n')\n",
    "                    print('Fehler im scrap') \n",
    "                    print(match)\n",
    "                    print(filenamePDF)\n",
    "                    #Check(filenamePDF)\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "     \n",
    "\n",
    "                RednerL.append(match)\n",
    "                del match\n",
    "                del Rede\n",
    "\n",
    "\n",
    "            #Extraction of the speech\n",
    "            RedenL = []\n",
    "            for Rede in Reden_rawL:\n",
    "                regReden = r'\\:(.*)'\n",
    "                matchReden = re.search(regReden, Rede, re.DOTALL)\n",
    "                RedenL.append(matchReden.group(1))     \n",
    "\n",
    "\n",
    "            #Test\n",
    "            regRedner = r'(\\([A-Za-zöäü]{3,12}\\)\\:(?!\\s\\.)|(Parl\\.\\s)?Staatssekretäri?n? [A-Za-zöäü\\-\\s]+\\:(?!\\s\\.)|^(?![A-zlöäüÜÄÖ]+\\-)(Vize)?(Alters)?P?p?räsidenti?n?\\s[A-Za-zÖÄÜüäöäßğéè \\-\\(\\)]+[a-z]+\\:)|(Bundes)?(Staats)?ministeri?n?[A-Za-zöäü\\-\\s]+\\:(?!\\s\\.)|Bundeskanzleri?n?\\:'\n",
    "            matchRedner = re.finditer(regRedner, plenarpro, re.MULTILINE)\n",
    "            Reden_TestL = []\n",
    "            for Rede in matchRedner:\n",
    "                Rede = Rede.group(0)\n",
    "                Reden_TestL.append(Rede)\n",
    "\n",
    "            #Data\n",
    "            WahlperiodeL = []\n",
    "            SitzungL = []\n",
    "            DatumL = []\n",
    "            regData = r'(\\d+)\\.\\sWahlperiode\\s(\\d+)\\.\\s\\w+\\s(\\d+\\.\\d+\\.\\d\\d\\d\\d)'\n",
    "            matchData = re.search(regData, filenamePDF)\n",
    "            Wahlperiode = matchData.group(1)\n",
    "            Sitzung = matchData.group(2)\n",
    "            Datum = matchData.group(3)\n",
    "\n",
    "\n",
    "            if (len(RedenL) != (len(Reden_TestL) - 1) or len(RednerL) != (len(Reden_TestL) - 1) or len(Reden_rawL) != (len(Reden_TestL) - 1)):\n",
    "                print('boom')\n",
    "                print(len(RedenL))\n",
    "                print(len(RednerL))\n",
    "                print(len(Reden_rawL))\n",
    "                print(len(Reden_TestL) - 1)\n",
    "\n",
    "                df = pd.DataFrame()\n",
    "                df['Redner'] =  RednerL \n",
    "                df['Reden'] = RedenL \n",
    "                df['Wahlperiode'] =  str(Wahlperiode)\n",
    "                df['Sitzung'] =  str(Sitzung)\n",
    "                df['Datum'] =  str(Datum)\n",
    "                \n",
    "                print(filenamePDF)\n",
    "\n",
    "                df.to_csv('Reden_einzel.csv')\n",
    "                #break\n",
    "\n",
    "                regRedner = r'(\\([A-Za-zöäü]{3,12}\\)\\:(?!\\s\\.)|(Parl\\.\\s)?Staatssekretäri?n? [A-Za-zöäü\\-\\s]+\\:(?!\\s\\.)|^(?![A-zlöäüÜÄÖ]+\\-)(Vize)?(Alters)?P?p?räsidenti?n?\\s[A-Za-zÖÄÜüäöäßğéè \\-\\(\\)]+[a-z]+\\:)|(Bundes)?(Staats)?ministeri?n?[A-Za-zöäü\\-\\s]+\\:(?!\\s\\.)|Bundeskanzleri?n?\\:'\n",
    "\n",
    "                for Rede in RedenL:\n",
    "                    matchRedner = re.finditer(regRedner, Rede, re.DOTALL | re.MULTILINE)\n",
    "\n",
    "                    for Rede_1 in matchRedner:\n",
    "                        pass\n",
    "                        print(Rede_1.group(0))\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "            df = pd.DataFrame()\n",
    "            df['Redner'] =  RednerL\n",
    "            df['Reden'] = RedenL\n",
    "            df['Wahlperiode'] =  str(Wahlperiode)\n",
    "            df['Sitzung'] =  str(Sitzung)\n",
    "            df['Datum'] =  str(Datum)\n",
    "            df_all = df_all.append(df, ignore_index = True)       \n",
    "\n",
    "\n",
    "\n",
    "            print(y)\n",
    "            y =y + 1  \n",
    "                \n",
    "#Export\n",
    "with open('df_all.pkl', 'wb') as f:\n",
    "        pickle.dump(df_all, f)\n",
    "df_all.to_csv('Reden.csv')\n",
    "print('Fertig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2777702c-c4b8-42d9-a789-08c0800eeb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 2: Clean-up\n",
    "df_all = pd.read_pickle('df_all.pkl')\n",
    "df_all_clean1 = df_all.copy()\n",
    "\n",
    "#Functions for cleaning up columns\n",
    "def text_cleaner(x):\n",
    "    x = re.sub(r'\\(Beifall( .*?)?\\)', '', x, flags=re.DOTALL  | re.MULTILINE)\n",
    "    x = re.sub(r'\\(Lachen .*?\\)', '', x, flags=re.DOTALL  | re.MULTILINE)\n",
    "    x = re.sub(r'\\(Zurufe? .*?\\)', '', x, flags=re.DOTALL  | re.MULTILINE)\n",
    "    x = re.sub(r'\\((Die Rednerin |Der Redner )(.*?)?\\)', '', x, flags=re.DOTALL  | re.MULTILINE)\n",
    "    x = re.sub(r'\\(Die Anwesenden (.*?)?\\)', '', x, flags=re.DOTALL  | re.MULTILINE)\n",
    "    x = re.sub(r'\\((\\[)?[A-ZLÖÄÜ][A-ZLÖÄÜa-züöäßğéè\\s\\-\\.\\]]+[[A-Za-zöäü]+\\].*?\\)', '', x, flags=re.DOTALL  | re.MULTILINE)\n",
    "    x = re.sub(r'^[Genau; herzlichen Sehr geehrter \\–Herr\\nPräsident! Liebe Kolleginnen\\! \\- Meine Damen und Herren! Liebe Kollegen! Herzlichen Dank\\.Zuschauerinnen und Zuschauer Werte verehrte Vizepräsidentin Frau Bundeskanzlerin Liebe Mitbürger hier auf den Tribünen Einen schönen guten Morgen Vor allem auch: Liebe Rentnerinnen und Rentner!]{0,150}\\!', '', x, flags=re.DOTALL  | re.MULTILINE)\n",
    "    return x\n",
    "    \n",
    "\n",
    "#Regex\n",
    "regBeifall = r'\\(Beifall( .*?)?\\)'\n",
    "regLachen = r'\\(Lachen .*?\\)'\n",
    "regZurufe = r'\\(Zurufe? .*?\\)'\n",
    "regKommentar = r'\\((\\[)?[A-ZLÖÄÜ][A-ZLÖÄÜa-züöäßğéè\\s\\-\\.\\]]+[[A-Za-zöäü]+\\].*?\\)'\n",
    "regDerDieRedner= r'\\((Die Rednerin |Der Redner )(.*?)?\\)'\n",
    "regAnwesenden = r'\\(Die Anwesenden (.*?)?\\)'\n",
    "\n",
    "#Listen\n",
    "BefälleL = []\n",
    "LachenL = []\n",
    "ZurufeL = []\n",
    "KommentarL = []\n",
    "DerDieRednerL = []\n",
    "AnwesendenL = []\n",
    "\n",
    "\n",
    "#Extraktion Beifall\n",
    "for Rede in df_all.loc[:,'Reden']:\n",
    "    Beifall_proRede = []\n",
    "    matchBeifall = re.finditer(regBeifall, Rede, re.DOTALL | re.MULTILINE)\n",
    "    try:\n",
    "        for Beifall_Rede in matchBeifall:\n",
    "            Beifall = Beifall_Rede.group(0)\n",
    "            Beifall = re.sub(r'\\(', '', Beifall, flags=re.DOTALL  | re.MULTILINE)\n",
    "            Beifall = re.sub(r'\\)', '', Beifall, flags=re.DOTALL  | re.MULTILINE)\n",
    "            Beifall_proRede.append(Beifall)\n",
    "    except:\n",
    "        Beifall_proRede = ''\n",
    "    BefälleL.append(Beifall_proRede)\n",
    "    \n",
    "\n",
    "\n",
    "#Extraktion Lachen\n",
    "for Rede in df_all.loc[:,'Reden']:\n",
    "    Lachen_proRede = []\n",
    "    matchLachen = re.finditer(regLachen, Rede, re.DOTALL | re.MULTILINE)\n",
    "    try:\n",
    "        for Lachen_Rede in matchLachen:\n",
    "            Lachen = Lachen_Rede.group(0)\n",
    "            Lachen = re.sub(r'\\(', '', Lachen, flags=re.DOTALL  | re.MULTILINE)\n",
    "            Lachen = re.sub(r'\\)', '', Lachen, flags=re.DOTALL  | re.MULTILINE)\n",
    "            Lachen_proRede.append(Lachen)\n",
    "    except:\n",
    "        Lachen_proRede = ''\n",
    "    LachenL.append(Lachen_proRede)\n",
    "    \n",
    "\n",
    "#Extraktion Zurufe\n",
    "for Rede in df_all.loc[:,'Reden']:\n",
    "    Zurufe_proRede = []\n",
    "    matchZurufe = re.finditer(regZurufe, Rede, re.DOTALL | re.MULTILINE)\n",
    "    try:\n",
    "        for Zurufe_Rede in matchZurufe:\n",
    "            Zurufe = Zurufe_Rede.group(0)\n",
    "            Zurufe = re.sub(r'\\(', '', Zurufe, flags=re.DOTALL  | re.MULTILINE)\n",
    "            Zurufe = re.sub(r'\\)', '', Zurufe, flags=re.DOTALL  | re.MULTILINE)\n",
    "            Zurufe_proRede.append(Zurufe)\n",
    "    except:\n",
    "        Zurufe_proRede = ''\n",
    "    ZurufeL.append(Zurufe_proRede)\n",
    "    \n",
    "\n",
    "#Extraktion Kommentar\n",
    "for Rede in df_all.loc[:,'Reden']:\n",
    "    Kommentar_proRede = []\n",
    "    matchKommentar = re.finditer(regKommentar, Rede, re.DOTALL | re.MULTILINE)\n",
    "    try:\n",
    "        for Kommentar_Rede in matchKommentar:\n",
    "            Kommentar = Kommentar_Rede.group(0)\n",
    "            Kommentar = re.sub(r'\\(', '', Kommentar, flags=re.DOTALL  | re.MULTILINE)\n",
    "            Kommentar = re.sub(r'\\)', '', Kommentar, flags=re.DOTALL  | re.MULTILINE)\n",
    "            Kommentar_proRede.append(Kommentar)\n",
    "    except:\n",
    "        Kommentar_proRede = ''\n",
    "    KommentarL.append(Kommentar_proRede)\n",
    "\n",
    "    \n",
    "#Extraktion DerDieRedner\n",
    "for Rede in df_all.loc[:,'Reden']:\n",
    "    DerDieRedner_proRede = []\n",
    "    matchDerDieRedner = re.finditer(regDerDieRedner, Rede, re.DOTALL | re.MULTILINE)\n",
    "    try:\n",
    "        for DerDieRedner_Rede in matchDerDieRedner:\n",
    "            DerDieRedner = DerDieRedner_Rede.group(0)\n",
    "            DerDieRedner = re.sub(r'\\(', '', DerDieRedner, flags=re.DOTALL  | re.MULTILINE)\n",
    "            DerDieRedner = re.sub(r'\\)', '', DerDieRedner, flags=re.DOTALL  | re.MULTILINE)\n",
    "            DerDieRedner_proRede.append(DerDieRedner)\n",
    "    except:\n",
    "        DerDieRedner_proRede = ''\n",
    "    DerDieRednerL.append(DerDieRedner_proRede)\n",
    "\n",
    "\n",
    "    \n",
    "#Extraktion DerDieRedner\n",
    "for Rede in df_all.loc[:,'Reden']:\n",
    "    Anwesenden_proRede = []\n",
    "    matchAnwesenden = re.finditer(regAnwesenden, Rede, re.DOTALL | re.MULTILINE)\n",
    "    try:\n",
    "        for Anwesenden_Rede in matchAnwesenden:\n",
    "            Anwesenden = Anwesenden_Rede.group(0)\n",
    "            Anwesenden = re.sub(r'\\(', '', Anwesenden, flags=re.DOTALL  | re.MULTILINE)\n",
    "            Anwesenden = re.sub(r'\\)', '', Anwesenden, flags=re.DOTALL  | re.MULTILINE)\n",
    "            Anwesenden_proRede.append(Anwesenden)\n",
    "    except:\n",
    "        Anwesenden_proRede = ''\n",
    "    AnwesendenL.append(Anwesenden_proRede)\n",
    "    \n",
    "    \n",
    "#Adding new columns\n",
    "df_all_clean1.loc[:,'Kommentar'] = KommentarL\n",
    "df_all_clean1.loc[:,'Zurufe'] = ZurufeL\n",
    "df_all_clean1.loc[:,'Lachen'] = LachenL\n",
    "df_all_clean1.loc[:,'Befälle'] = BefälleL\n",
    "df_all_clean1.loc[:,'Anwesende'] = AnwesendenL\n",
    "df_all_clean1.loc[:,'DerDieRedner'] = DerDieRednerL\n",
    "\n",
    "\n",
    "    \n",
    "#Cleaning all extractions\n",
    "df_all_clean1.loc[:,'Reden_clean'] = df_all.loc[:,'Reden'].apply(text_cleaner)\n",
    "with open('df_all_clean1.pkl', 'wb') as f:\n",
    "    pickle.dump(df_all_clean1, f)\n",
    "df_all_clean1.to_csv('Reden_clean1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f179c0b7-5ff7-4c59-97f4-92644e3010ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_clean1 = pd.read_pickle('df_all_clean1.pkl')\n",
    "df_all_clean2 = df_all_clean1.copy()\n",
    "\n",
    "\n",
    "#Counting the letters of the speeches\n",
    "def counter(letter):\n",
    "    letter = re.sub(r'\\s', '', letter, flags=re.DOTALL  | re.MULTILINE)\n",
    "    letter = re.sub(r'\\n', '', letter, flags=re.DOTALL  | re.MULTILINE)\n",
    "    letter_count = len(letter)\n",
    "    return letter_count\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "#Daily points\n",
    "def Tagespunkte(top):\n",
    "    TopL = []\n",
    "    regTop = r'rufe den Tagesordnungspunkt \\d+'\n",
    "  \n",
    "    try:\n",
    "        matchTop = re.finditer(regTop, top, flags=re.DOTALL  | re.MULTILINE)\n",
    "        for Top in matchTop:\n",
    "            Top_ = Top.group(0)\n",
    "\n",
    "    except:\n",
    "        Top_ = ''\n",
    "    \n",
    "    TopL.append(Top_)\n",
    "    return TopL\n",
    "    \n",
    "\n",
    "\n",
    "#Extract subjects\n",
    "for Redner_subject in ['AfD','DIELINKE', 'CDUCSU', 'FDP', 'SPD', 'Grüne', 'fraktionslos', 'Präsident', 'Vizepräsident', 'Bürgermeister', 'Staatssekretär', 'Bundesminister', 'Staatsminister', 'Ministerpräsident']:\n",
    "\n",
    "    def subject(sub):\n",
    "        if Redner_subject == 'AfD' or Redner_subject == 'DIELINKE' or Redner_subject == 'CDUCSU' or Redner_subject == 'FDP' or Redner_subject == 'SPD' or Redner_subject ==  'Grüne' or Redner_subject == 'fraktionslos':\n",
    "            regSub = r'\\((.*?)\\)'\n",
    "            matchSub = re.search(regSub, sub)\n",
    "            try:\n",
    "                matchSub = matchSub.group(1)\n",
    "                if matchSub == str(Redner_subject):\n",
    "                    sub_text = 1\n",
    "                else:\n",
    "                    sub_text = 0\n",
    "            except:\n",
    "                sub_text = 0\n",
    "            \n",
    "        if Redner_subject == 'Präsident' or  Redner_subject == 'Vizepräsident':\n",
    "            \n",
    "      \n",
    "            if sub.startswith(Redner_subject):\n",
    "                \n",
    "                sub_text = 1\n",
    "            else:\n",
    "                sub_text = 0\n",
    "        \n",
    "        if Redner_subject == 'Ministerpräsident' or Redner_subject == 'Bürgermeister' or Redner_subject == 'Staatsminister' or Redner_subject == 'Staatssekretär' or Redner_subject == 'Bundesminister':\n",
    "            \n",
    "            if Redner_subject in sub:\n",
    "\n",
    "                sub_text = 1\n",
    "            else:\n",
    "                sub_text = 0\n",
    "\n",
    "    \n",
    "        return sub_text\n",
    "            \n",
    "        \n",
    "\n",
    "    \n",
    "    df_all_clean2.loc[:,str(Redner_subject)] = df_all_clean2.loc[:,'Redner'].apply(subject)\n",
    "\n",
    "    \n",
    "\n",
    "#Length of speeches\n",
    "df_all_clean2.loc[:,'letter_count'] = df_all_clean2.loc[:,'Reden'].apply(counter)\n",
    "\n",
    "#Create sample\n",
    "df_all_clean2.loc[:,'TOP'] = df_all_clean2.loc[:,'Vizepräsident'].apply(Tagespunkte)\n",
    "df_all_clean2.loc[:,'TOP'] = df_all_clean2.loc[:,'Präsident'].apply(Tagespunkte)\n",
    "\n",
    "with open('df_all_clean2.pkl', 'wb') as f:\n",
    "    pickle.dump(df_all_clean2, f)\n",
    "df_all_clean2.to_csv('Reden_clean2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c15fc7-d775-4ddf-b428-4c75ada8ebc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sample erstellen\n",
    "df_all_clean2 = pd.read_pickle('df_all_clean2.pkl')\n",
    "df_all_clean3 = df_all_clean2.copy()\n",
    "\n",
    "df_parteien = df_all_clean3.loc[df_all_clean3['Vizepräsident'] ==0 ]\n",
    "df_parteien = df_parteien.loc[df_parteien['Präsident'] ==0 ]\n",
    "df_parteien = df_parteien.loc[df_parteien['fraktionslos'] ==0 ]\n",
    "df_parteien = df_parteien.loc[df_parteien['letter_count'] >= 668 ]\n",
    "\n",
    "\n",
    "print(len(df_parteien))\n",
    "print(len(df_all_clean3))\n",
    "df_parteien.to_csv('df_parteien.csv')\n",
    "with open('df_parteien.pkl', 'wb') as f:\n",
    "    pickle.dump(df_parteien, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d995ad7-9e80-4b01-b6d0-157865dcc368",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show length of speeches\n",
    "%matplotlib inline \n",
    "fig, ax = plt.subplots(figsize=[11, 8]) \n",
    "df_parteien.loc[:,'letter_count'].sort_values().plot(kind='bar', ax=ax)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Kernel3.9)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
